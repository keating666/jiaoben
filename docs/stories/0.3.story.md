# Story 0.3: Implement Tongyi API Serverless Function

## Status
Draft

## Story
**As a** system integrator,
**I want** to deploy Tongyi (通义千问) API functionality as a Vercel Serverless Function,
**so that** external applications can access our advanced text generation and script rewriting services via HTTP endpoints

## Acceptance Criteria
1. Create `/api/tongyi/text-generation` endpoint for general text generation
2. Create `/api/tongyi/script-rewrite` endpoint specifically for video script rewriting
3. Implement proper authentication using API keys from request headers
4. Support streaming responses for better user experience
5. Handle errors gracefully with appropriate HTTP status codes
6. Implement request validation with clear error messages
7. Response times meet performance requirements (< 8s for script rewriting)
8. Endpoints support both synchronous and streaming response modes

## Tasks / Subtasks
- [ ] **Task 1: Create Tongyi API endpoints structure** (AC: 1, 2)
  - [ ] Create api/tongyi/text-generation.ts file
  - [ ] Create api/tongyi/script-rewrite.ts file
  - [ ] Set up shared utilities for Tongyi API handling
  - [ ] Configure TypeScript types for request/response
  
- [ ] **Task 2: Implement text generation endpoint** (AC: 1, 3, 5, 6)
  - [ ] Parse and validate JSON request body
  - [ ] Implement system prompt configuration
  - [ ] Call Tongyi API using existing client
  - [ ] Handle both completion and streaming modes
  - [ ] Transform errors to user-friendly messages
  
- [ ] **Task 3: Implement script rewrite endpoint** (AC: 2, 3, 5, 6)
  - [ ] Parse original script and target style parameters
  - [ ] Construct appropriate prompts for script rewriting
  - [ ] Support multiple rewrite styles (humorous, educational, etc.)
  - [ ] Preserve script structure in output
  - [ ] Implement character count validation
  
- [ ] **Task 4: Add streaming support** (AC: 4, 8)
  - [ ] Implement Server-Sent Events (SSE) for streaming
  - [ ] Handle stream interruptions gracefully
  - [ ] Add streaming/non-streaming mode toggle
  - [ ] Ensure proper cleanup of resources
  
- [ ] **Task 5: Security and performance** (AC: 3, 7)
  - [ ] Implement API key validation
  - [ ] Add request size limits (max 4000 characters)
  - [ ] Implement request queuing for rate limiting
  - [ ] Add performance monitoring
  - [ ] Cache frequently used prompts
  
- [ ] **Task 6: Testing and documentation** (AC: 7, 8)
  - [ ] Create integration tests for both modes
  - [ ] Test with various script lengths and styles
  - [ ] Document streaming vs non-streaming usage
  - [ ] Create example client code

## Dev Notes

### Tongyi API Integration
[Source: tech-validation/scripts/tongyi-text-generation.ts]
- Existing client supports Qwen models
- Built-in retry logic and error handling
- Supports both streaming and completion modes
- Average response time: 2.7 seconds

### Required Environment Variables
[Source: tech-validation/.env.example]
```
TONGYI_API_KEY=your_api_key
TONGYI_BASE_URL=https://dashscope.aliyuncs.com/api/v1
TONGYI_MODEL=qwen-max
```

### API Endpoint Specifications

#### Text Generation Endpoint
```
POST /api/tongyi/text-generation
Content-Type: application/json
Authorization: Bearer <api-key>

Request:
{
  "messages": [
    {
      "role": "system" | "user" | "assistant",
      "content": string
    }
  ],
  "model": string (optional, default: "qwen-max"),
  "max_tokens": number (optional, default: 1500),
  "temperature": number (optional, default: 0.7),
  "stream": boolean (optional, default: false)
}

Response (non-streaming):
{
  "success": boolean,
  "data": {
    "content": string,
    "usage": {
      "prompt_tokens": number,
      "completion_tokens": number,
      "total_tokens": number
    },
    "model": string
  },
  "error": string (optional)
}

Response (streaming):
Server-Sent Events with format:
data: {"content": "partial text...", "done": false}
data: {"content": "complete text", "done": true, "usage": {...}}
```

#### Script Rewrite Endpoint
```
POST /api/tongyi/script-rewrite
Content-Type: application/json
Authorization: Bearer <api-key>

Request:
{
  "original_script": string (required, max 2000 chars),
  "style": "humorous" | "educational" | "emotional" | "professional",
  "target_length": number (optional, similar to original),
  "additional_requirements": string (optional),
  "stream": boolean (optional, default: false)
}

Response:
{
  "success": boolean,
  "data": {
    "rewritten_script": string,
    "style_applied": string,
    "character_count": number,
    "changes_summary": string,
    "processing_time": number
  },
  "error": string (optional)
}
```

### Script Rewriting Prompts
[Source: tech-validation test results]
```typescript
const stylePrompts = {
  humorous: "请将这段短视频脚本改写成幽默风趣的风格，加入适当的网络流行语和段子...",
  educational: "请将这段脚本改写成教育科普风格，确保信息准确、逻辑清晰...",
  emotional: "请将这段脚本改写成情感丰富的风格，增加感人元素...",
  professional: "请将这段脚本改写成专业正式的风格，使用行业术语..."
};
```

### Performance Optimization
[Source: Architecture requirements]
- Implement connection pooling for Tongyi API
- Cache common prompt templates
- Use streaming for responses > 500 characters
- Implement request batching where possible

### Error Handling
[Source: tech-validation implementation]
- Rate limit errors (429): Implement exponential backoff
- Authentication errors (401): Return clear message
- Model overload (503): Automatic retry with fallback
- Timeout errors: Set 30s timeout for script rewriting

## Testing
### Testing Standards
- Test with scripts of various lengths (100-2000 characters)
- Verify all style transformations maintain content accuracy
- Test streaming interruption and resumption
- Validate character count limits
- Test concurrent request handling

### Performance Benchmarks
- Text generation: < 3 seconds for 500 tokens
- Script rewriting: < 8 seconds for 1000 characters
- Streaming first token: < 1 second
- Concurrent requests: Support 10 simultaneous

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-23 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record
### Agent Model Used
_To be populated during implementation_

### Debug Log References
_To be populated during implementation_

### Completion Notes List
_To be populated during implementation_

### File List
_To be populated during implementation_

## QA Results
_To be populated after implementation_